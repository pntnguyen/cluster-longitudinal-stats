[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Longitudinal and Clustered Data",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#clustered-and-longitudinal-analysis",
    "href": "index.html#clustered-and-longitudinal-analysis",
    "title": "Analysis of Longitudinal and Clustered Data",
    "section": "Clustered and longitudinal analysis",
    "text": "Clustered and longitudinal analysis\nIndividual level RCT: randomize individuals to control group and treatment group and measure the outcome for both groups. Vast majority of data analysis of RCTs and observational data uses some from of regression (linear, logistic, generalized linear models, …)\nSimplest case is linear regression with single covariate, and using ordinary least squares (OLS) regression method:\n\\[\nY_i = \\beta_{0} + \\beta_{1}X_i+e_i\n\\]\nwhere \\(e_i\\) is random error with variance \\(\\sigma^2\\). This method assumes that errors \\(e_i\\) are independent of (uncorrelated with) each other:\n\\[\nCov(e_i,e_j) = 0 \\ when \\ i \\neq j\n\\]\nHowever, clustered and longitudinal data often be expected to be correlation with group (cluster):\n\nindividuals may belong to clusters such as families, schools, medical centers. (clustered)\nthe ‘units’ are not individuals but are repeated measures of the same variable on an individual over time. (longitudinal)\n\n=&gt; It is not make sense to assume the errors in the regression model are all uncorrelated with each other\n=&gt; If using OLS regression methods (ignoring the correlation structure) to those type of data will generally product unbiased estimates of effect but:\n\nIncorrect standard error (SE) =&gt; poor inference (more important)\nThe estimates of effects (i.e. regression coefficients) may be more variable (less efficient) than they could be",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#example-study-design-with-correlated-data",
    "href": "index.html#example-study-design-with-correlated-data",
    "title": "Analysis of Longitudinal and Clustered Data",
    "section": "Example study design with correlated data",
    "text": "Example study design with correlated data\n\nCross-over trial\n\nPros:\n\nWithin-individual comparison - variability of outcome for treatment effect reduced because less variability within- than between-individuals\nFewer participants needed than a parallel group design\n\nCons:\nCarry over effect of the intervention (design assumes minimal carry over effect)\nParticipants drop out after 1st treatment and don’t receive 2nd treatment\nGenerally, only suitable for:\n\nParticipants with conditions or diseases that are chronic or relatively stable\nShort-term outcomes\nInterventions with short term impact, so washout period is feasible\n\n\n\nLongitudinal data\nIn this practical we will use data from a study investigating recovery following appendectomy in children. The aim of the study was to determine whether the children who underwent a laparoscopic appendectomy achieved a faster rate of recovery than children who underwent conventional “open” appendectomy.\nIn this study a measure of recovery was made using an “uptimer”, a device worn by the children on their thigh. The uptimer senses the position of the thigh and records the times of changes from a horizontal position to a “vertical” position (at least 45 degrees from the horizontal). An increasing number of position changes (horizontal to vertical) from one day to the next is a marker of recovery.\nData were obtained from 29 children aged between 8 and 15 years, 18 of whom underwent laparoscopic appendectomy and 11 underwent open appendectomy. The children were not randomly assigned to the types of operation – the decision was made by the surgeon on call at the time of presentation. The uptime data was intended to be recorded for each child from day two post-operatively and thereafter, however some missing data resulted. In this practical we consider days two to five post-surgery.\n\nlibrary(haven)\nlibrary(ggplot2)\nappendix &lt;- read_dta(\"./data/appendix.dta\")\n\nappendix$lognchanges = log(appendix$nchanges)\nappendix$group &lt;- factor(appendix$group,labels = c(\"Lap\",\"Open\"))\nggplot(appendix, aes(day,lognchanges,group=patid))+\n  geom_point()+\n  geom_line()+\n  facet_wrap(~group)+\n  theme_minimal()+\n  labs(y = \"log number of changes\")\n\n\n\n\n\n\n\n\n\n\nClustered Randomised trials\n\nCluster randomised trials are experiments in which clusters of individuals (e.g. schools, villages, general practices) rather than independent individuals are randomly allocated to intervention groups\nPotential reasons include:\n\nIntervention naturally applied at the cluster level (e.g. Effect of water and environment revitalisation in informal settlements in Indonesia and Fiji (RISE))\nTo avoid treatment group contamination (e.g. education program vs usual care to patients in a general practice)\nApplying the intervention at the cluster level is more feasible than at the individual level (e.g. intervention at a school)\nEthical considerations\nTo enhance participant compliance\n\nUnit of randomisation: cluster\nUnit of outcome measure: individual\n\nObservations on participants in the same cluster tend to be correlated (intracluster correlation)\nSample size for a cluster randomized trial needs to be greater than an individually randomized trial\nSample size needs to be inflated by ‘design effect’ which depends on intracluster correlation and average cluster size. (Note, it is better to have a large number of clusters with less participants per cluster, than a small number of clusters with many participants per cluster)\n\n\n\n\n\n\n\nNote\n\n\n\nThe analysis of outcome measures at the individual participant level need to take account of clustering",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#how-to-choose-method-of-analysis",
    "href": "index.html#how-to-choose-method-of-analysis",
    "title": "Analysis of Longitudinal and Clustered Data",
    "section": "How to choose method of analysis",
    "text": "How to choose method of analysis\nBased on:\n\nData structure\nResearch question\n\n\nComparing within cluster\nWhen analysis question involves comparisons within clusters, considering the clustering increases precision of estimation (lower standard errors, smaller P-values). Because we are removing a source of variation from the comparison\nData structure examples:\n\nLongitudinal studies (collect the same measurement overtime, and compare within individuals)\nCross over-trials\n\n\n\nComparing between clusters\nWhen analysis question involves comparisons between clusters, considering the clustering decreases precision of estimation (higher standard errors, higher P-values). Because positive intra-cluster correlation reduces the amount of independent information provided by individuals\nData structure examples:\n\nCluster trials",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "brief_intro.html",
    "href": "brief_intro.html",
    "title": "1  Brief introduction",
    "section": "",
    "text": "1.1 Comparison within cluster\nA small cross-over trial of asthma medications in children aged 7-14 years. Outcome is the peak expiratory flow.\nResearch question:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Brief introduction</span>"
    ]
  },
  {
    "objectID": "brief_intro.html#sec-cwc",
    "href": "brief_intro.html#sec-cwc",
    "title": "1  Brief introduction",
    "section": "",
    "text": "Is there evidence that mean peak flow is different between treatment and control?\n\n\n1.1.1 Data\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nsub\nSubject identifier\n\n\npef0\nPeak flow measurement under control\n\n\npef1\nPeak flow measurement under treatment\n\n\nseq\nOrder in which treatments were received (1 = treatment followed by control; 2 = control followed by treatment)\n\n\n\n\nlibrary(haven)\npef &lt;- read_dta(\"./data/pef.dta\")\npef\n\n# A tibble: 10 × 4\n     sub  pef0  pef1 seq                \n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt;          \n 1     1   330   335 1 [Treatment first]\n 2     2   270   310 2 [Control first]  \n 3     3   350   360 2 [Control first]  \n 4     4   240   290 1 [Treatment first]\n 5     5   320   330 1 [Treatment first]\n 6     6   360   350 2 [Control first]  \n 7     7   340   380 1 [Treatment first]\n 8     8   280   335 2 [Control first]  \n 9     9   365   370 1 [Treatment first]\n10    10   320   340 2 [Control first]  \n\n\n\n\n1.1.2 Ignoring the paired structure\nWhen we are not consider the paired outcome, and using independence two-sample t-test, we will obtain\n\nlibrary(tidyr); library(dplyr)\nlibrary(ggplot2)\npef.long &lt;- pivot_longer(pef, \n                         cols = c(\"pef0\", \"pef1\"), \n                         names_to = \"trt\", \n                         values_to = \"pef\") \n\npef.long$trt &lt;- ifelse(pef.long$trt==\"pef0\",\"control\",\"treatment\")\n\n\n# two ways to implement two samples independent t-test in R\n## t-test function\nt.test(pef ~ trt, data=pef.long,var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  pef by trt\nt = -1.4387, df = 18, p-value = 0.1674\nalternative hypothesis: true difference in means between group control and group treatment is not equal to 0\n95 percent confidence interval:\n -55.35666  10.35666\nsample estimates:\n  mean in group control mean in group treatment \n                  317.5                   340.0 \n\n## using linear regression\nsummary(lm(pef~trt,data=pef.long))\n\n\nCall:\nlm(formula = pef ~ trt, data = pef.long)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-77.50 -15.00   2.50  24.38  47.50 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    317.50      11.06  28.711   &lt;2e-16 ***\ntrttreatment    22.50      15.64   1.439    0.167    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.97 on 18 degrees of freedom\nMultiple R-squared:  0.1031,    Adjusted R-squared:  0.05331 \nF-statistic:  2.07 on 1 and 18 DF,  p-value: 0.1674\n\n\n\nconfint(lm(pef~trt,data=pef.long), level=0.95)\n\n                 2.5 %    97.5 %\n(Intercept)  294.26684 340.73316\ntrttreatment -10.35666  55.35666\n\n\n\nThe estimated treatment effect is 340-317.5 = 22.5 (95% CI = [-10.36, 55.36])\nStandard error is 15.64\n\n\n\n1.1.3 Consider the paired structure\n\nt.test(pef$pef1,pef$pef0,paired=T)\n\n\n    Paired t-test\n\ndata:  pef$pef1 and pef$pef0\nt = 3.2134, df = 9, p-value = 0.0106\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n  6.660412 38.339588\nsample estimates:\nmean difference \n           22.5 \n\nsd(pef$pef1-pef$pef0)/sqrt(10) # standard error of estimate \n\n[1] 7.001984\n\n\nSame estimated treatment error is 22.5, but more precise with lower standard error 7.0019838\n\n\n\n\n\n\nNoteConclusion\n\n\n\nWhen using independent two-sample t-test, the test treat observation from the same subject as independent observations. It not valid, because in this data there is the correlation between observation from the same subject (paired data).\n\n\n\n\n1.1.4 When some pairs have missing data ?\nSuppose some paired have one ‘half’ missing data: are these cases still informative?\nIn this section, we just stop at introduce the linear mixed effects model, when using linear mixed effects model for the paired data without missing data, it equal using paired t-test\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + S_i + e_{ij}\n\\]\nWhere:\n\n\\(\\beta_0\\): the population expected response in control group\n\\(\\beta_1\\): treatment different\n\\(S_i\\): systematic differents between individual (assumed ~ N(0,\\(\\sigma_s^2\\)))\n\\(e_{ij}\\): within individual error (assumed ~ N(0,\\(\\sigma_e^2\\)))\n\nThe output of the model includes: fixed effect (\\(\\beta_0,\\beta_1\\)), random effect (\\(\\sigma_s^2,\\sigma_e^2\\))\n\n\nlibrary(lme4)                  # mixed effects models\n\n## fit lme model for peak flow data\n\nfit.lmer1 &lt;- lmer(pef ~ trt + (1|sub), data = pef.long)\nsummary(fit.lmer1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: pef ~ trt + (1 | sub)\n   Data: pef.long\n\nREML criterion at convergence: 174.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.33176 -0.45244  0.00889  0.58564  1.22464 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n sub      (Intercept) 977.8    31.27   \n Residual             245.1    15.66   \nNumber of obs: 20, groups:  sub, 10\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)   317.500     11.059  28.711\ntrttreatment   22.500      7.002   3.213\n\nCorrelation of Fixed Effects:\n            (Intr)\ntrttreatmnt -0.317\n\nconfint(fit.lmer1, level=0.95)\n\n                  2.5 %    97.5 %\n.sig01        18.086710  51.50392\n.sigma        10.131426  24.80399\n(Intercept)  295.142588 339.85741\ntrttreatment   8.124216  36.87578\n\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\nThe estimated treatment effect (mean difference, treatment vs control) and its standard error are:\n\nEstimated treatment effect = 22.5\nSE = 7.002\n\nThese are exactly the same numbers as those obtained for the paired t-test. (N.B. You may see a difference of sign, simply because the t-test and mixed model estimation treat the two groups in the opposite order when defining the comparison.) So in the case of fully observed paired data, the mixed model (estimated by the “REML” method) is equivalent to the paired t-test.\n\n\nUsing gtsummary and broom package for summary table of model output\n\nlibrary(gtsummary)\nmodel &lt;- lmer(pef ~ trt + (1|sub), data = pef.long)  \n\nmodel |&gt;\n  tbl_regression(\n    intercept = TRUE,\n    tidy_fun = broom.mixed::tidy,\n    estimate_fun = purrr::partial(style_ratio, digits = 3)\n  ) |&gt;\n  as_gt() |&gt;\n  gt::summary_rows(\n    groups = NULL,\n    columns = everything(),\n    fns = list(\n      \"Variance (subject)\"  = ~ sprintf(\"%.3f\", as.data.frame(VarCorr(model))$vcov[1]),\n      \"Variance (residual)\" = ~ sprintf(\"%.3f\", as.data.frame(VarCorr(model))$vcov[2])\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n\n\n\n\n\n(Intercept)\n317.5\n295.8, 339.2\n\n\n\ntrt\n\n\n\n\n\n\n\n    control\n—\n—\n\n\n\n    treatment\n22.50\n8.776, 36.22\n\n\n\nsub.sd__(Intercept)\n31.27\n\n\n\n\n\nResidual.sd__Observation\n15.66\n\n\n\n\nVariance (subject)\n977.778\n977.778\n977.778\n\n\nVariance (residual)\n245.139\n245.139\n245.139\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nThe output of this model:\n\n\\(\\beta_0\\): (Intercept)\n\\(\\beta_1\\): treatment in trt labels\n\\(\\sigma_s\\): sub.sd_(Intercept)\n\\(\\sigma_e\\): Residual.sd_Observation\n\\(\\sigma_s^2\\): Variance (subject)\n\\(\\sigma_e^2\\): Variance (residual)\n\n\nEstimating the ICC (intra class correlation)\n\n\\(\\sigma_s^2\\) = 977.78: represents the between-subject variation, i.e. the variability in “underlying” peak flow between subjects.\n\\(\\sigma_e^2\\) = 245.14: represents the within-subject variation, i.e. the variability among peak flow measurements on the same person (under the same treatment)\n\nThe (estimated) intraclass correlation coefficient is:\n\\[ICC = \\frac{977.78}{977.78+245.14} = 0.7995\\]\n\n\n\n\n\n\nNoteExplanation of why accounting for correlation in comparisons within a cluster increases the precision of estimation\n\n\n\nLinear mixed model and paired t-test\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + S_i + e_{ij}\n\\] For an individual\n\\[\n\\begin{align}\nY_{i1} - Y_{i0} &= (\\beta_0 + \\beta_1 X_{i1} + S_i + e_{i1}) - (\\beta_0 + \\beta_1 X_{i0} + S_i + e_{i0}) \\\\\n&= \\beta_0 + \\beta_1 X_{i1} + S_i + e_{i1} - \\beta_0 - \\beta_1 X_{i0} - S_i - e_{i0} \\\\\n&= \\beta_1 + e_{i1} - e_{i0}\n\\end{align}\n\\]\nAveraged over all individuals:\n\n\\(\\widehat{\\beta} = \\frac{\\sum\\nolimits_{N}^{i = 1} (Y_{i1} - Y_{i0})}{N} = \\overline{Y_1} - \\overline{Y_0}\\)\n\\(Var(\\widehat{\\beta}) = \\frac{2\\sigma_e^2}{N}\\)\n\nTwo samples t-test\n\n\\(\\widehat{\\beta} = \\frac{\\sum\\nolimits_{i = 1}^{N} (Y_{i1} - Y_{i0})}{N} = \\overline{Y_1} - \\overline{Y_0}\\)\n\\(Var(\\widehat{\\beta}) = Var(\\overline{Y_1}) + Var(\\overline{Y_0})\\)\n\nVariance in each group is the average of the total variation of observations about treatment group mean: \\(Var(\\overline{Y_1}) = \\frac{(\\sigma_s^2 + \\sigma_e^2)}{N}\\).\nSo \\(Var(\\widehat{\\beta}) = 2 \\times \\frac{(\\sigma_s^2 + \\sigma_e^2)}{N}\\)\n=&gt; The variance will be higher and the precision of estimation will be lower",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Brief introduction</span>"
    ]
  },
  {
    "objectID": "brief_intro.html#comparison-between-clusters",
    "href": "brief_intro.html#comparison-between-clusters",
    "title": "1  Brief introduction",
    "section": "1.2 Comparison between clusters",
    "text": "1.2 Comparison between clusters\nCluster randomize trials: All members of each cluster receive the same treatment\n\nAnti-malarial trial (Lec 1): Cluster = village, treatment given to all individuals in a village\nComparison of treatment involves between cluster comparisons\nAnalysis estimates difference between (population) mean of treatment and control groups\n\nMarginal models\nConsider the treatment group only in a cluster RCT;\ni refers to a cluster, \\(Y_{ij}\\) is the outcome for a cluster i with \\(j^{th}\\) measurement of outcome\n\\[\n\\begin{align}\nE[Y_{ij}] = \\overline{Y_i} = \\mu \\quad\\quad  Var(Y_{ij}) = \\sigma_T^2 \\quad\\quad Corr(Y_{ij},Y_{ik}) = \\rho\n\\end{align}\n\\]\nThis is called a marginal model or population average model which specifies only the mean, variance, and covariance\nVariance of cluster mean for a cluster of size \\(n_i\\):\n\\(Var(\\overline{Y_i}) = \\frac{\\sigma^2_T (1+(n_i - 1)\\rho)}{n_i}\\)\nEach cluster mean estimates \\(\\mu\\)\n\\[\n\\begin{align}\n\\widehat{\\mu} = \\frac{\\sum\\nolimits_{i = 1}^{N} w_i \\overline{Y_i}}{\\sum\\nolimits_{i = 1}^{N} w_i} \\quad where \\ w_i = \\frac{n_i}{1+(n_i-1)\\rho}\n\\end{align}\n\\]\nThe generalized estimating equations (GEE) can be written as\n\\[\n\\sum\\nolimits_{i = 1}^{N} w_i(\\overline{Y_i} - \\widehat{\\mu}) = 0\n\\]\n\n\n\n\n\n\nNoteConclusion\n\n\n\n\nGEE estimate population mean of each treatment arm, this model take correlation structure into account but treats it as a nuisance\nVariance of sample mean INCREASES with (positively) correlated observations (LESS precise), see \\(Var(\\overline{Y_i}) = \\frac{\\sigma^2_T (1+(n_i - 1)\\rho)}{n_i}\\), \\(\\rho = Corr(Y_{ij},Y_{ik})\\)\nCan be used to estimate population average odds ratios, rate ratios, etc.\n\n\n\n\n1.2.1 Example data\nThe Melbourne Water Quality Study was undertaken to assess whether filtering the household water supply for viruses, bacteria and protozoa was associated with a reduced burden of gastroenteritis. The study was a cluster-randomised double blind clinical trial comparing real versus sham water filters. The primary outcome was gastroenteritis during a 68 week follow-up period.\nWe will use a subset of the data consisting of 2437 individuals in 533 families. Because we have so far only explored methods for continuous data, we will examine a secondary outcome measure – total water consumption.\n\nwater &lt;- read_dta(\"./data/water.dta\")\n\nwater$logwater &lt;- log(water$watertot)\n\nwater |&gt; \n  mutate(filter = factor(filter,labels = c(\"Control\",\"Filter\"))) |&gt; \n  filter(house &lt;= 20) |&gt; \n  ggplot(aes(x = as.factor(house), y = logwater))+\n  geom_point() +\n  facet_wrap(~filter)+\n  labs(x = \"House\", y  = \"Log of total water consumption\")+\n  theme_bw()\n\n\n\n\nThe plot shows that the log of each house’s water consumption was randomly assigned to the treatment and control groups. In this plot, we see the first 20 houses of the dataset.\n\n\n\n\nUse a GEE approach to estimate the effect of the filter on water consumption\nNote that the corstr argument will be explained more detail in Section 2.4\n\nlibrary(geepack)\n\nfit.geeglm2 &lt;- geeglm(logwater ~ filter,\n                      data = water, \n                      id = house, \n                      corstr = \"exchangeable\")\nsummary(fit.geeglm2)\n\n\nCall:\ngeeglm(formula = logwater ~ filter, data = water, id = house, \n    corstr = \"exchangeable\")\n\n Coefficients:\n            Estimate Std.err     Wald Pr(&gt;|W|)    \n(Intercept)  0.99718 0.02821 1249.198   &lt;2e-16 ***\nfilter       0.06083 0.03781    2.588    0.108    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = exchangeable \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)   0.3878 0.01943\n  Link = identity \n\nEstimated Correlation Parameters:\n      Estimate Std.err\nalpha   0.3236 0.02639\nNumber of clusters:   533  Maximum cluster size: 9 \n\nconfint.default(fit.geeglm2)\n\n               2.5 % 97.5 %\n(Intercept)  0.94189 1.0525\nfilter      -0.01328 0.1349\n\n\nThe output of the model (on the logscale of water consumption):\n\nThe estimated treatment effect 0.061 (-0.013,0.135)\nThe standard error 0.038\n\nWe need to exponentiate the estimation, using gtsummary package to provide the summary table\n\nlibrary(gtsummary)\n\nfit.geeglm2 |&gt;\n  tbl_regression(\n    intercept = TRUE,\n    exponentiate = TRUE,\n    tidy_fun = broom.mixed::tidy,\n    estimate_fun = purrr::partial(style_ratio, digits = 2)\n  )\n\n\n\n\n\n\n\nCharacteristic\nexp(Beta)\n95% CI\np-value\n\n\n\n\n(Intercept)\n2.71\n2.56, 2.86\n&lt;0.001\n\n\nRandomised group (filter/control)\n1.06\n0.99, 1.14\n0.11\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n=&gt; That means the difference in the real filter group is 6% greater than in the sham filter group (with 95% CI from a 1% decrease to a 14% increase). The p-value is p=0.11. So there is very minimal evidence of an effect of the real filter on changing mean water consumption.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Brief introduction</span>"
    ]
  },
  {
    "objectID": "brief_intro.html#summary",
    "href": "brief_intro.html#summary",
    "title": "1  Brief introduction",
    "section": "Summary",
    "text": "Summary\nCorrelation structure of your data can impact on your standard error estimates\n\nWhen the comparison is within cluster/individual like in a cross-over trial, accounting for clustering improve precision (smaller standard errors)\nWhen the comparison is between clusters like in a cluster RCT, modelling clustering reduces precision\n\nLinear mixed models (conditional models) produce estimates taking individual variation into account\nGEE models (marginal models) produced population average estimates, but need many clusters",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Brief introduction</span>"
    ]
  },
  {
    "objectID": "gee.html",
    "href": "gee.html",
    "title": "2  Generalized Estimating Equation",
    "section": "",
    "text": "2.1 Data description\nPotthoff and Roy growth data:\nlibrary(haven)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(mice)\nlibrary(ggcorrplot)\nlibrary(gtsummary)\npotthoffroy |&gt; \n  pivot_longer(cols = -c(id,sex)) |&gt; \n  mutate(age = str_extract(name,'\\\\d+') |&gt; as.numeric()) |&gt; \n  ggplot(aes(x = age,y = value))+\n  geom_line(aes(group = id))+\n  facet_wrap(~sex)+\n  theme_bw()\nWe consider the correlation of the value in male and female group",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Estimating Equation</span>"
    ]
  },
  {
    "objectID": "gee.html#data-description",
    "href": "gee.html#data-description",
    "title": "2  Generalized Estimating Equation",
    "section": "",
    "text": "Growth measurements for 27 children (11 girls and 16 boys)\nDistance (in mm) from the centre of the pituitary gland to the pterygo-maxillary fissure recorded at ages 8, 10, 12 and 14 years.\nInterested in the differences between males and females in pattern of growth over the 4 time points (from age 8 to 14 years).\n\n\n\n\n\n\n\npotthoffroy |&gt; \n  filter(sex == \"M\") |&gt; \n  select(-c(id,sex)) |&gt; \n  cor(use=\"pairwise.complete.obs\")|&gt; \n  ggcorrplot(lab = TRUE) +\n  labs(tag = \"Male\")\n\n\n\n\n\n\n\n\n\n\npotthoffroy |&gt; \n  filter(sex == \"F\") |&gt; \n  select(-c(id,sex)) |&gt; \n  cor(use=\"pairwise.complete.obs\")|&gt; \n  ggcorrplot(lab = TRUE) +\n  labs(tag = \"Female\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\nPositive correlations within-cluster. Greater for Females than Males.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Estimating Equation</span>"
    ]
  },
  {
    "objectID": "gee.html#fitting-ols",
    "href": "gee.html#fitting-ols",
    "title": "2  Generalized Estimating Equation",
    "section": "2.2 Fitting OLS",
    "text": "2.2 Fitting OLS\n\ndata &lt;- potthoffroy |&gt; \n  pivot_longer(cols = -c(id,sex)) |&gt; \n  mutate(age = str_extract(name,'\\\\d+') |&gt; as.numeric(),\n         sex = factor(sex,labels = c(0,1))) \n\nlm_model &lt;- lm(value ~ sex + age, data = data)\n\nsummary(lm_model)\n\n\nCall:\nlm(formula = value ~ sex + age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9882 -1.4882 -0.0586  1.1916  5.3711 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.38569    1.12857  13.633  &lt; 2e-16 ***\nsex1         2.32102    0.44489   5.217 9.20e-07 ***\nage          0.66019    0.09776   6.753 8.25e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.272 on 105 degrees of freedom\nMultiple R-squared:  0.4095,    Adjusted R-squared:  0.3983 \nF-statistic: 36.41 on 2 and 105 DF,  p-value: 9.726e-13\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPretend observations independent – WRONG!!\nThe estimation of beta (sex, age, intercept) is unbiased, but the standard errors are incorrect if the covariance is not independent.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\\(\\widehat{\\beta}\\) are estimated using Odinary Least Square\n\\(Var(\\widehat{\\beta})\\) - derived from covariance matrix of \\(Y\\), known as V\nCovariance = variance \\(\\times\\) correlation\nConventional regression model fit by OLS has \\(V_i = \\sigma^2I_{n_i \\times n_i}\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Estimating Equation</span>"
    ]
  },
  {
    "objectID": "gee.html#how-can-we-estimate-the-correct-variance-of-the-ols-estimator-when-the-data-are-not-independent",
    "href": "gee.html#how-can-we-estimate-the-correct-variance-of-the-ols-estimator-when-the-data-are-not-independent",
    "title": "2  Generalized Estimating Equation",
    "section": "2.3 How can we estimate the correct variance of the OLS estimator when the data are not independent?",
    "text": "2.3 How can we estimate the correct variance of the OLS estimator when the data are not independent?\nRobust / empirical / “sandwich” variance estimator:\n\nUse actual correlations observed in the data (i.e. empirical correlations)\nRobust – for a large number of clusters it converges to the correct variance of the OLS parameter estimate, irrespective of what the true covariance V is, as long as the corrected model for the mean of Y has been specified.\nSandwich: the data driven empirical estimator is the “meat” in the sandwich between two model-based terms (“slices of bread”)\n\n\nlibrary(lmtest)\nlibrary(sandwich)\n\nlm_model &lt;- lm(value ~ sex + age, data = data)\n\n## Robust step\ncov_cl &lt;- vcovCL(lm_model, cluster = data$id)\ncov_cl\n\n            (Intercept)       sex1          age\n(Intercept)  0.87447024 -0.3160789 -0.050590613\nsex1        -0.31607886  0.5948969 -0.006492700\nage         -0.05059061 -0.0064927  0.005173734\n\nse_robust_model &lt;- coeftest(lm_model, vcov = cov_cl)\n\nse_robust_model\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 15.385690   0.935131 16.4530 &lt; 2.2e-16 ***\nsex1         2.321023   0.771296  3.0093  0.003279 ** \nage          0.660185   0.071929  9.1783 4.268e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare\n\n\n\nlm_model |&gt; \ntbl_regression(\n  intercept = TRUE,\n  estimate_fun = purrr::partial(style_ratio, digits = 3)) |&gt; \n  remove_row_type(\n    type = \"reference\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n15.39\n13.15, 17.62\n&lt;0.001\n\n\nsex\n\n\n\n\n\n\n\n\n    1\n2.321\n1.439, 3.203\n&lt;0.001\n\n\nage\n0.660\n0.466, 0.854\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nse_robust_model |&gt; \n  tbl_regression(\n    intercept = TRUE,\n    estimate_fun = purrr::partial(style_ratio, digits = 3)\n  ) \n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\n(Intercept)\n15.39\n13.53, 17.24\n&lt;0.001\n\n\nsex1\n2.321\n0.792, 3.850\n0.003\n\n\nage\n0.660\n0.518, 0.803\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nSex: difference in mean outcome between sexes at any age = between-subject comparison\n\nSE with robust &gt; SE assuming independence (0.7712956 &gt; 0.4448862)\nBetween cluster comparison is LESS precise with positive within-subject correlation\n\nAge: effect estimated within-subject.\n\nSE with robust &lt; SE independence ( 0.0719287 &lt; 0.0977589)\nWithin-cluster comparison is MORE precise with positive correlation\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\nAs we can see, within-cluster comparison is MORE precise with positive correlation. By modelling the covariance V correctly using the robust variance estimator instead of assuming independence, we generally obtain smaller standard errors. How to model covariance V?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Estimating Equation</span>"
    ]
  },
  {
    "objectID": "gee.html#sec-gee",
    "href": "gee.html#sec-gee",
    "title": "2  Generalized Estimating Equation",
    "section": "2.4 Common correlation / covariance structures",
    "text": "2.4 Common correlation / covariance structures\n\nExchangeable: correlation between any two measurements on an individual is the same no matter how far apart in time they are.\nAutoregressive: correlation decreases with distance\nUnstructured: no pre-determined structure\n\n\nPutting together in GEE\nThe covariance V is unknown, but take a “guest” at its form.\nCall this W (e.g. exchangeable, autoregressive, etc.)\n\nCan get idea from empirical correlations\nCalled a “working” covariance – we know it may be wrong but we work with it anyway\n\nStep for GEE:\n\nFit OLS regression, and estimate parameters in W using residuals from OLS regression\nSolve weighted least squares regression using \\(\\widehat{W}\\) to get \\(\\widehat{\\beta}_W\\)\nIterate between \\(\\widehat{W}\\) and \\(\\widehat{\\beta}_W\\) until convergence",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Estimating Equation</span>"
    ]
  },
  {
    "objectID": "gee.html#example-data",
    "href": "gee.html#example-data",
    "title": "2  Generalized Estimating Equation",
    "section": "2.5 Example data",
    "text": "2.5 Example data\nData from a study investigating recovery following appendectomy in children. The aim of the study was to determine whether the children who underwent a laparoscopic appendectomy achieved a faster rate of recovery than children who underwent conventional “open” appendectomy.\nIn this study a measure of recovery was made using an “uptimer”, a device worn by the children on their thigh. The uptimer senses the position of the thigh and records the times of changes from a horizontal position to a “vertical” position (at least 45 degrees from the horizontal). An increasing number of position changes (horizontal to vertical) from one day to the next is a marker of recovery.\n\nappendix &lt;- read_dta(\"./data/appendix.dta\")\n\nappendix &lt;- appendix |&gt; \n  mutate(lognchanges = log(nchanges),\n         group = factor(group,labels = c(\"Lap\",\"Open\")))  \n\nPlot the distribution of number of change\n\nappendix |&gt; \n  ggplot(aes(nchanges))+\n  geom_histogram()\n\n\n\n\n\n\n\n\nThis seem like a skewed distribution, so we will use logarithm\n\nappendix |&gt; \n  ggplot(aes(lognchanges))+\n  geom_histogram()\n\n\n\n\n\n\n\n\nAnd we will see the description of the data\n\nappendix |&gt; \n  ggplot(aes(x = day, y = lognchanges))+\n  geom_line(aes(group = patid))+\n  geom_point()+\n  facet_wrap(~group)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nUsing GEE with different working covariance \\(W\\)\nThe research question is whether the rate of change of the outcome variable – logged number of position changes – differs between the two groups. In order to address this within a regression modeling framework, we need to model the relationship between the outcome and time, and allow this relationship to differ between groups. The effect of surgical group will then be measured by a single interaction parameter.\n\nlibrary(geepack); library(gee)\n\n\nIndependenceExchangeableAutoregressive\n\n\n\ngeefit.ind &lt;- geeglm(lognchanges~day*group,\n                     data = appendix,  \n                     id = patid, \n                     corstr = \"independence\")\nsummary(geefit.ind)\n\n\nCall:\ngeeglm(formula = lognchanges ~ day * group, data = appendix, \n    id = patid, corstr = \"independence\")\n\n Coefficients:\n              Estimate  Std.err    Wald Pr(&gt;|W|)    \n(Intercept)    3.74588  0.29367 162.700  &lt; 2e-16 ***\nday            0.36338  0.07159  25.763 3.86e-07 ***\ngroupOpen     -0.04868  0.50811   0.009    0.924    \nday:groupOpen -0.11807  0.11090   1.133    0.287    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = independence \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)   0.4488 0.08769\nNumber of clusters:   29  Maximum cluster size: 4 \n\n\n\n\n\ngeefit.ex &lt;- geeglm(lognchanges~day*group,\n                      data = appendix, \n                      id = patid, \n                      corstr = \"exchangeable\")\nsummary(geefit.ex)\n\n\nCall:\ngeeglm(formula = lognchanges ~ day * group, data = appendix, \n    id = patid, corstr = \"exchangeable\")\n\n Coefficients:\n              Estimate Std.err   Wald Pr(&gt;|W|)    \n(Intercept)     3.8441  0.2724 199.13  &lt; 2e-16 ***\nday             0.3407  0.0676  25.41  4.6e-07 ***\ngroupOpen      -0.2163  0.4333   0.25     0.62    \nday:groupOpen  -0.0794  0.1007   0.62     0.43    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = exchangeable \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)    0.449   0.088\n  Link = identity \n\nEstimated Correlation Parameters:\n      Estimate Std.err\nalpha    0.668  0.0988\nNumber of clusters:   29  Maximum cluster size: 4 \n\n\n\n\n\ngeefit.ar1 &lt;- geeglm(lognchanges~day*group,\n                      data = appendix, \n                      id = patid, \n                      corstr = \"ar1\",wave=day)\nsummary(geefit.ar1)\n\n\nCall:\ngeeglm(formula = lognchanges ~ day * group, data = appendix, \n    id = patid, waves = day, corstr = \"ar1\")\n\n Coefficients:\n              Estimate Std.err   Wald Pr(&gt;|W|)    \n(Intercept)     3.7648  0.2494 227.89  &lt; 2e-16 ***\nday             0.3539  0.0656  29.12  6.8e-08 ***\ngroupOpen      -0.2869  0.4374   0.43     0.51    \nday:groupOpen  -0.0613  0.1015   0.36     0.55    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = ar1 \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)     0.45  0.0882\n  Link = identity \n\nEstimated Correlation Parameters:\n      Estimate Std.err\nalpha     0.76   0.079\nNumber of clusters:   29  Maximum cluster size: 4 \n\n\n\n\n\n\n\n\n\nIndependence\nExchangeable\nAuto-regressive 1\n\n\n\n\nGroup effect (on slope)\n-0.1181\n-0.0794\n-0.0613\n\n\nRobust SE of group effect (on slope)\n0.1109\n0.1007\n0.1015",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Generalized Estimating Equation</span>"
    ]
  },
  {
    "objectID": "linear_mixed.html",
    "href": "linear_mixed.html",
    "title": "3  Linear mixed model",
    "section": "",
    "text": "3.1 Data\nLongitudinal data: Potthoff and Roy (1964):\nlibrary(mice)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\ndata &lt;- potthoffroy |&gt; \n  pivot_longer(cols = -c(id,sex)) |&gt; \n  mutate(age = str_extract(name,'\\\\d+') |&gt; as.numeric(),\n         sex = factor(sex,labels = c(\"Female\",\"Male\")),\n         time = age-8,\n         y = value)\n\ndata |&gt; \n  ggplot(aes(x = time, y = y))+\n  geom_line(aes(group = id))+\n  geom_point()+\n  facet_wrap(~sex)+\n  labs(x = \"Years since age 8\", y = \"Distance\")+\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear mixed model</span>"
    ]
  },
  {
    "objectID": "linear_mixed.html#data",
    "href": "linear_mixed.html#data",
    "title": "3  Linear mixed model",
    "section": "",
    "text": "Measured distance from centre of the pituitary gland to the pterygomaxillary fissure by x-ray (important in orthodontics)\nMeasured for boys and girls with aim to understand changes with age and by sex\nMeasured at ages 8, 10, 12, 14\nRescaled to show time since age 8",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear mixed model</span>"
    ]
  },
  {
    "objectID": "linear_mixed.html#random-intercepts-model",
    "href": "linear_mixed.html#random-intercepts-model",
    "title": "3  Linear mixed model",
    "section": "3.2 Random intercepts model",
    "text": "3.2 Random intercepts model\nIn this example, we only consider the male group, and focus on within cluster comparison (change over time). In linear mixed model, we extend to compare the variation between individuals or subjects by adding one level to the simple linear regression \\(Y_j = \\beta_0 + \\beta_1 t_j\\).\nThe “two level” or “hierarchical” model:\n\nLevel 1: Describes variation within individuals\n\nStraight line (linear regression) for each subject\n\\(\\beta_{0i}\\) is subject-specific intercept\n\\(\\beta_{1}\\) is the common slope\n\nLevel 2: Describes variation in intercepts between individuals\n\n\\(\\beta_{0i} = \\beta_0 + b_{0i}\\) with \\(b_{0i} \\sim N(0,\\sigma^2_{b0})\\)\n\\(\\beta_{0}\\) is average intercept\n\n\nThe combined model: \\(Y_{ij} = (\\beta_0 + b_{0i}) + \\beta_1 t_{ij} + e_{ij}, \\ e_{ij} \\sim N(0,\\sigma^2_e)\\)\nThen the marginal model for Y will be:\n\n\\([Y_{ij}] = \\beta_0 + \\beta_1 t_{ij}\\)\n\\(Var(Y_{ij}) = \\sigma^2_{b0} + \\sigma^2_e\\)\n\\(Corr(Y_{ij},Y_{ik}) = \\frac{\\sigma^2_{b0}}{\\sigma^2_{b0} + \\sigma^2_{e}}\\)\n\nIn words:\n\nAverage growth is linear with slope β1\nVariances are constant over time (and equal to the sum of the variance of the intercept and the error)\nCorrelation between different observations from the same individuals is constant (exchangeable correlation structure)\n\nR code\n\nlibrary(lme4)\n\nrandom_intecept_model &lt;- lmer(y ~ time + (1 | id),data = data, subset = (sex == \"Male\"))\n\nsummary(random_intecept_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ time + (1 | id)\n   Data: data\n Subset: (sex == \"Male\")\n\nREML criterion at convergence: 273.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.00805 -0.64069  0.00783  0.53448  3.05295 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 2.641    1.625   \n Residual             2.816    1.678   \nNumber of obs: 64, groups:  id, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 22.61563    0.53690  42.123\ntime         0.78438    0.09382   8.361\n\nCorrelation of Fixed Effects:\n     (Intr)\ntime -0.524\n\n\nPlotting how random intercept model fit the data\n\ndata |&gt; \n  filter(sex == \"Male\") |&gt; \n  mutate(fitted = predict(random_intecept_model)) |&gt; \n  ggplot(aes(x = time, y = fitted))+\n  geom_line(aes(group = id))+\n  geom_point(aes(y = value))+\n  labs(x = \"Years since age 8\", y = \"Distance\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSummarise the output\n\nlibrary(gtsummary)\n\n\nrandom_intecept_model |&gt; \n    tbl_regression(\n      intercept = TRUE,\n      tidy_fun = broom.mixed::tidy,\n      estimate_fun = purrr::partial(style_ratio, digits = 3)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n\n\n\n\n(Intercept)\n22.62\n21.56, 23.67\n\n\ntime\n0.784\n0.601, 0.968\n\n\nid.sd__(Intercept)\n1.625\n\n\n\n\nResidual.sd__Observation\n1.678\n\n\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nModel output:\n\n\\(\\beta_0 = 22.62\\)\n\\(\\beta_1 = 0.784\\)\n\\(b_{0i} \\sim N(0,\\sigma^2_{b0}), \\ SD: \\sigma_{b0} = 1.625, \\ \\sigma^2_{b0} = 2.641\\)\n\\(e_{ij} \\sim N(0,\\sigma^2_e), \\ SD: \\sigma_e = \\ 1.678, \\ \\sigma^2_e = 2.816\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear mixed model</span>"
    ]
  },
  {
    "objectID": "linear_mixed.html#random-intercepts-and-slopes-model",
    "href": "linear_mixed.html#random-intercepts-and-slopes-model",
    "title": "3  Linear mixed model",
    "section": "3.3 Random intercepts and slopes model",
    "text": "3.3 Random intercepts and slopes model\nBased on random intercepts models \\(Y_{ij} = (\\beta_0 + b_{0i}) + \\beta_1 t_{ij} + e_{ij}\\). We extend the slopes \\(\\beta_1 = \\beta_1 + b_{1i}\\):\n\nLevel 1: Describes variation within individuals\n\n\\(Y_{ij} = \\beta_{0i} + \\beta_{1i}t_{ij} + e_{ij}\\)\nStraight line (linear regression) for each subject\n\\(\\beta_{0i}\\) is individual-specific intercept\n\\(\\beta_{1i}\\) is the individual-specific slope\n\nLevel 2: Describes variation in intercepts AND slopes between individuals\n\nAlso allows correlation between them\n\\(\\beta_{0i} = \\beta_{00}+b_{0i}\\), with \\(b_{0i} \\sim N(0,\\sigma^2_{b0})\\)\n\\(\\beta_{1i} = \\beta_{10}+b_{1i}\\), with \\(b_{0i} \\sim N(0,\\sigma^2_{b1})\\)\n\\(Corr(b_{0i},b_{1i}) = \\rho_b\\)\n\n\nCombined model: \\(Y_{ij} = (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i})t_{ij} + e_{ij}, \\ e_{ij} \\sim N(0,\\sigma^2_e)\\)\nR code\n\nrandom_slope_intecept_model &lt;- lmer(y ~ time + (1 + time | id),data = data, \n                                    subset = (sex == \"Male\"))\n\nsummary(random_slope_intecept_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ time + (1 + time | id)\n   Data: data\n Subset: (sex == \"Male\")\n\nREML criterion at convergence: 273.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.65543 -0.59382  0.01946  0.57552  3.15773 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 3.04625  1.7454        \n          time        0.03562  0.1887   -0.34\n Residual             2.58906  1.6091        \nNumber of obs: 64, groups:  id, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  22.6156     0.5511  41.041\ntime          0.7844     0.1016   7.722\n\nCorrelation of Fixed Effects:\n     (Intr)\ntime -0.558\n\n\nPlotting how random intercept model fit the data\n\ndata |&gt; \n  filter(sex == \"Male\") |&gt; \n  mutate(fitted = predict(random_slope_intecept_model)) |&gt; \n  ggplot(aes(x = time, y = fitted))+\n  geom_line(aes(group = id))+\n  geom_point(aes(y = value))+\n  labs(x = \"Years since age 8\", y = \"Distance\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSummarise the output\n\nrandom_slope_intecept_model |&gt; \n    tbl_regression(\n      intercept = TRUE,\n      tidy_fun = broom.mixed::tidy,\n      estimate_fun = purrr::partial(style_ratio, digits = 3)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n\n\n\n\n(Intercept)\n22.62\n21.54, 23.70\n\n\ntime\n0.784\n0.585, 0.983\n\n\nid.sd__(Intercept)\n1.745\n\n\n\n\nid.cor__(Intercept).time\n-0.339\n\n\n\n\nid.sd__time\n0.189\n\n\n\n\nResidual.sd__Observation\n1.609\n\n\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteModel output and interpretation\n\n\n\n\n\\(\\beta_0 = 22.62\\): Average value at age 8 = 22.6\n\\(\\beta_1 = 0.784\\): Average rate of growth is 0.784 mm/ year\n\\(b_{0i} \\sim N(0,\\sigma^2_{b0}), \\ SD: \\sigma_{b0} = \\ 1.745, \\sigma^2_{b0} = 3.046\\): Variations in intercepts between individuals\n\\(b_{1i} \\sim N(0,\\sigma^2_{b1}), \\ SD: \\sigma_{b1} = \\ 0.189, \\sigma^2_{b1} = 0.036\\): Variations in slopes between individuals\n\\(Corr(b_{0i},b_{1i}) = \\rho_b = -0.339\\): Correlation between value at age 8 and later time points is -0.339\n\\(e_{ij} \\sim N(0,\\sigma^2_e), \\ SD: \\sigma_e = \\ 1.609, \\sigma^2_e = 2.589\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear mixed model</span>"
    ]
  },
  {
    "objectID": "linear_mixed.html#differences-between-groups",
    "href": "linear_mixed.html#differences-between-groups",
    "title": "3  Linear mixed model",
    "section": "3.4 Differences between groups",
    "text": "3.4 Differences between groups\nIn the previous section, we only consider the male group, and focus on within cluster comparison (change over time). In this section, we will explore the following question:\n\nIs average rate of growth the same for males and females?\nWhat is the variability in rate of growth within each sex?\nIs rate of growth correlated with size/distance at age 8?\n\nWe fit random intercepts and slopes model, and allow subject-specific intercept and slope to depend on sex\n\nLevel 1: Describes variation within individuals\n\n\\(Y_{ij} = \\beta_{0i} + \\beta_{1i}t_{ij} + e_{ij}\\)\n\nLevel 2: Describes variation in intercepts and slopes between individuals\n\n\\(\\beta_{0i} = \\beta_{00} + b_{0i} + \\beta_{01}Sex\\)\n\\(\\beta_{1i} = \\beta_{10} + b_{1i} + \\beta_{11}Sex\\)\n\n\nCombined model: \\(Y_{ij} = (\\beta_{00} + b_{0i} + \\beta_{01}Sex) + (\\beta_{10} + b_{1i} + \\beta_{11}Sex)t_{ij} + e_{ij}\\)\nR code\n\nrandom_slope_intecept_model2 &lt;- lmer(y ~ sex*time + (1 + time | id),data = data)\n\nsummary(random_slope_intecept_model2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ sex * time + (1 + time | id)\n   Data: data\n\nREML criterion at convergence: 432.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1681 -0.3859  0.0071  0.4452  3.8495 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept) 3.23394  1.7983        \n          time        0.03252  0.1803   -0.09\n Residual             1.71621  1.3100        \nNumber of obs: 108, groups:  id, 27\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)   21.2091     0.6350  33.401\nsexMale        1.4065     0.8249   1.705\ntime           0.4795     0.1037   4.623\nsexMale:time   0.3048     0.1347   2.262\n\nCorrelation of Fixed Effects:\n            (Intr) sexMal time  \nsexMale     -0.770              \ntime        -0.396  0.305       \nsexMale:tim  0.305 -0.396 -0.770\n\n\nPlotting\n\ndata |&gt; \n  mutate(fitted = predict(random_slope_intecept_model2)) |&gt; \n  ggplot(aes(x = time, y = fitted))+\n  geom_line(aes(group = id))+\n  geom_point(aes(y = value))+\n  facet_wrap(~sex)+\n  labs(x = \"Years since age 8\", y = \"Distance\")+\n  theme_bw()\n\n\n\n\n\n\n\n\nSummary table\n\nrandom_slope_intecept_model2 |&gt; \n    tbl_regression(\n      intercept = TRUE,\n      tidy_fun = broom.mixed::tidy,\n      estimate_fun = purrr::partial(style_ratio, digits = 3)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n\n\n\n\n(Intercept)\n21.21\n19.96, 22.45\n\n\nsex\n\n\n\n\n\n\n    Female\n—\n—\n\n\n    Male\n1.407\n-0.210, 3.023\n\n\ntime\n0.480\n0.276, 0.683\n\n\nsex * time\n\n\n\n\n\n\n    Male * time\n0.305\n0.041, 0.569\n\n\nid.sd__(Intercept)\n1.798\n\n\n\n\nid.cor__(Intercept).time\n-0.091\n\n\n\n\nid.sd__time\n0.180\n\n\n\n\nResidual.sd__Observation\n1.310\n\n\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\n\n\n\n\n\n\nModel\n\\[Y_{ij} = (\\beta_{00} + b_{0i} + \\beta_{01}Sex) + (\\beta_{10} + b_{1i} + \\beta_{11}Sex)t_{ij} + e_{ij}\\]\n\n\n\n\n\n\nNoteModel output and interpretation\n\n\n\n\n\\(\\beta_{11} = 0.305\\): Different in growth rate of Male vs Female\n\\(\\beta_{00} + \\beta_{10} = 0.48\\): Average growth rates when sex = 0 (Female): 0.48, Male (0.48+0.3) = 0.78\n\\(b_{1i} \\sim N(0,\\sigma^2_{b1}), \\ SD: \\sigma_{b1} = \\ 0.18, \\sigma^2_{b1} = 0.03\\):\n\n95% of Female have growth rate 0.48 +/- 1.96*0.18 = 0.13 to 0.83\n95% of Male have growth rate 0.78 +/- 1.96*0.18 = 0.43 to 1.13\n\n\\(Corr(b_{0i},b_{1i}) = \\rho_b = -0.09\\): Little correlation between initial size at age 8 and growth rate between ages 8-14 years (assumes common correlation for males and females)\n\n\n\n\ndata |&gt; \n  mutate(fitted = predict(random_slope_intecept_model2),\n         pred.pop = predict(\n           random_slope_intecept_model2,\n           re.form = NA   \n         )) |&gt; \n  ggplot(aes(x = time))+\n  geom_line(aes(y = pred.pop,group = id),size = 1)+\n  geom_line(aes(y = value,group = id),alpha = .5)+\n  geom_point(aes(y = value))+\n  facet_wrap(~sex)+\n  labs(x = \"Years since age 8\", y = \"Distance\")+\n  theme_bw()\n\n\n\n\nThe lines that connected the points are the raw data. The bold lines are the outcome of the model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear mixed model</span>"
    ]
  },
  {
    "objectID": "binary.html",
    "href": "binary.html",
    "title": "4  GEE and GLMM for binary outcome",
    "section": "",
    "text": "4.1 Summary GEE and LMM for continuous outcomes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GEE and GLMM for binary outcome</span>"
    ]
  },
  {
    "objectID": "binary.html#data",
    "href": "binary.html#data",
    "title": "4  GEE and GLMM for binary outcome",
    "section": "4.2 Data",
    "text": "4.2 Data\nData from a longitudinal study of adolescents in the state of Victoria carried out between August 1992 and July 1995. Approximately 2,000 adolescents were recruited and followed up at six timepoints (“waves”) at six-monthly intervals. Some of the participants were not recruited until the second wave\nWe will focus on one of the smoking measures: regular smoking. Smoking behaviour was determined at each wave using a 7-day retrospective diary, completed by all participants except those who considered themselves non-smokers or ex-smokers (no cigarette in the previous month). From the diary response, a subject was categorized as a “regular smoker” if they reported smoking on at least six days of the previous week.\nWe are interested in determining the prevalence of smoking for each gender and investigating how this changes with age. Secondary questions concern the association between the prevalence of smoking and baseline factors such as parental smoking.\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nid\nParticipant identifier\n\n\nage\nAge of participant at survey wave (years)\n\n\nwave\nWave of data collection (1,2,…,6)\n\n\nsex\nSex (0=male, 1=female)\n\n\nparsmk\nParental smoking (1=at least one parent smokes most days, 0=otherwise)\n\n\nregsmoke\nRegular smoker (1=participant reports smoking at least 6 days a week, 0=otherwise)\n\n\n\n\nlibrary(foreign)\nsmoke_prev &lt;- read.dta(\"data/smoke_prev.dta\")\nhead(smoke_prev)\n\n      id school wave      age  sex born_oz parsmk regsmoke   wgt_mv wgt_str\n1 920006   3010    1 14.17488 male       1      0        0 2.142857 0.77829\n2 920006   3010    2 14.73357 male       1      0        0 1.120879 0.77829\n3 920006   3010    3 15.17214 male       1      0        0 1.115974 0.77829\n4 920006   3010    4 15.65571 male       1      0        0 1.183295 0.77829\n5 920006   3010    5 16.12012 male       1      0        0 1.217184 0.77829\n6 920006   3010    6 16.67762 male       1      0        0 1.256158 0.77829\n  sregion\n1     InU\n2     InU\n3     InU\n4     InU\n5     InU\n6     InU\n\n\nLabel sex variable\nBefore analysis, we should center variables by subtracting the sample mean from each observation because:\n\nIt makes the intercept more interpretable by creating a meaningful zero point; when the coefficient is negative, it is below the population mean, and when the coefficient is positive, it is above the population mean.\n\n\n## centering age and wave variables\nsmoke_prev &lt;- transform(smoke_prev, age_c = age - 16.2,wave_c = (wave - 3.5)/2)\n\n\nlibrary(patchwork)\nlibrary(ggplot2)\n\np1 &lt;-smoke_prev |&gt; \n  ggplot(aes(x = wave, y = age))+\n  geom_line(aes(group = id))+\n  labs(title = \"Before centering\")\n\np2 &lt;- smoke_prev |&gt; \n  ggplot(aes(x = wave_c, y = age_c))+\n  geom_line(aes(group = id))+\n  labs(title = \"After centering\")\n\np1|p2\n\n\n\n\nWe can see the different in x and y axes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GEE and GLMM for binary outcome</span>"
    ]
  },
  {
    "objectID": "interaction_term.html",
    "href": "interaction_term.html",
    "title": "Explain interaction term in regression",
    "section": "",
    "text": "Using iris data for example\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nWe want to fit a multivariate linear regression with Sepal.Length as the outcome variable and Sepal.Width and Species as predictors. What is the difference between Sepal.Length ~ Sepal.Width + Species and Sepal.Length ~ Sepal.Width * Species?\nThe second formula includes an interaction term. When we use an interaction term, we assume that the effect of sepal width differs across species; that is, the slope of sepal width is different for each species. When we use the first formula, we assume that the effect of sepal width is the same across all species.\nR code to ilustrate this\n\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n## no interaction between width and species assumption\nlm1  &lt;- lm(Sepal.Length ~ Sepal.Width + Species, data=iris)\n\np1 &lt;- iris |&gt; \n  mutate(pred = predict(lm1)) |&gt; \n  ggplot(aes(x = Sepal.Width, y = pred))+\n  geom_line()+\n  facet_wrap(~Species)+\n  theme_bw()+\n  scale_y_continuous(limits = c(4,8))\n\n## interaction between width and species assumption\nlm2  &lt;- lm(Sepal.Length ~ Sepal.Width*Species, data=iris)\n\np2 &lt;- iris |&gt; \n  mutate(pred = predict(lm2)) |&gt; \n  ggplot(aes(x = Sepal.Width, y = pred))+\n  geom_line()+\n  facet_wrap(~Species)+\n  theme_bw()+\n  scale_y_continuous(limits = c(4,8))\n\np1/\n  p2\n\n\n\n\nThe upper figure show the outcome of the linear regression assumes no interaction between width and species. The bottom figure show the outcome of the linear regression assumes interaction between width and species.\n\n\n\n\n\n\n\n\n\n\nNoteConclusion\n\n\n\nAs we can see, the slope for versicolor in the bottom figure clearly shows a difference between species, whereas the slopes for the three species in the top figure are the same.",
    "crumbs": [
      "Explain interaction term in regression"
    ]
  },
  {
    "objectID": "binary.html#fitting-normal-logistic-regression-ignoring-within-subject-correlation.",
    "href": "binary.html#fitting-normal-logistic-regression-ignoring-within-subject-correlation.",
    "title": "4  GEE and GLMM for binary outcome",
    "section": "4.3 Fitting normal logistic regression, ignoring within-subject correlation.",
    "text": "4.3 Fitting normal logistic regression, ignoring within-subject correlation.\n\nglm.fit1 &lt;- glm(regsmoke ~ sex*age_c, data = smoke_prev, family =binomial)\nsummary(glm.fit1)\n\n\nCall:\nglm(formula = regsmoke ~ sex * age_c, family = binomial, data = smoke_prev)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -2.07908    0.05028 -41.349  &lt; 2e-16 ***\nsexfemale        0.25049    0.06599   3.796 0.000147 ***\nage_c            0.26608    0.05901   4.509 6.52e-06 ***\nsexfemale:age_c  0.06319    0.07735   0.817 0.413962    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6728.6  on 8788  degrees of freedom\nResidual deviance: 6647.5  on 8785  degrees of freedom\nAIC: 6655.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nUsing gtsummary to create summary table, in this code we used exponential = TRUE to delog the odds ratio\n\nlibrary(gtsummary)\n\nglm.fit1 |&gt; \n  tbl_regression(exponentiate = TRUE,\n                 intercept = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\n(Intercept)\n0.13\n0.11, 0.14\n&lt;0.001\n\n\nsex\n\n\n\n\n\n\n\n\n    male\n—\n—\n\n\n\n\n    female\n1.28\n1.13, 1.46\n&lt;0.001\n\n\nage_c\n1.30\n1.16, 1.47\n&lt;0.001\n\n\nsex * age_c\n\n\n\n\n\n\n\n\n    female * age_c\n1.07\n0.92, 1.24\n0.4\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nInterpret the coefficients\n\nIntercept: the odds of being a regular smoker for the reference group (males) at the reference value of age (age_c = 0)\nsex: Difference in odds of regular smoking between females and males at age_c = 0\nage_c: Change in odds of regular smoking per 1-unit increase in age (centered) among reference group ( sex = 0 = males)\nsex * age_c : Difference in the age effect between males and females (effect modification)\n\nTo calculate the change in odds of regular smoking per 1-unit in age among females, we should take one step, using esticon() function\n\nlibrary(doBy) \n\nesticon(glm.fit1, c(0, 0, 1, 1)) |&gt; exp()\n\n       estimate  std.error  statistic    p.value      beta0     df\n[1,] 1.3900e+00 1.0513e+00 6.7831e+18 1.0000e+00 1.0000e+00 2.7183\n\n\nThe logistic regression above is equaled model which directly gives both age effects for male and females\n\nglm.fit1.1 &lt;- glm(regsmoke ~ sex+I(age_c*(sex==\"male\"))+I(age_c*(sex==\"female\")), \n                  data = smoke_prev, family =binomial)\n\nsummary(glm.fit1.1)\n\n\nCall:\nglm(formula = regsmoke ~ sex + I(age_c * (sex == \"male\")) + I(age_c * \n    (sex == \"female\")), family = binomial, data = smoke_prev)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  -2.07908    0.05028 -41.349  &lt; 2e-16 ***\nsexfemale                     0.25049    0.06599   3.796 0.000147 ***\nI(age_c * (sex == \"male\"))    0.26608    0.05901   4.509 6.52e-06 ***\nI(age_c * (sex == \"female\"))  0.32927    0.05000   6.585 4.55e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6728.6  on 8788  degrees of freedom\nResidual deviance: 6647.5  on 8785  degrees of freedom\nAIC: 6655.5\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nglm.fit1.1 |&gt; \n  tbl_regression(exponentiate = TRUE,\n                 intercept = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\n(Intercept)\n0.13\n0.11, 0.14\n&lt;0.001\n\n\nsex\n\n\n\n\n\n\n\n\n    male\n—\n—\n\n\n\n\n    female\n1.28\n1.13, 1.46\n&lt;0.001\n\n\nI(age_c * (sex == \"male\"))\n1.30\n1.16, 1.47\n&lt;0.001\n\n\nI(age_c * (sex == \"female\"))\n1.39\n1.26, 1.53\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\nIntercept: the odds of being a regular smoker for the reference group (males) at the reference value of age (age_c = 0)\nsex: Difference in odds of regular smoking between males and females at age_c = 0\nCoefficient at (third line): Change in odds of regular smoking per 1-unit increase in age (centered) among males\nCoefficient at (fourth line): Change in odds of regular smoking per 1-unit increase in age (centered) among females",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GEE and GLMM for binary outcome</span>"
    ]
  },
  {
    "objectID": "binary.html#incorporate-correlation-in-outcome-data-gee",
    "href": "binary.html#incorporate-correlation-in-outcome-data-gee",
    "title": "4  GEE and GLMM for binary outcome",
    "section": "4.4 Incorporate correlation in outcome data: GEE",
    "text": "4.4 Incorporate correlation in outcome data: GEE\n\nUnlike the normal distribution, binomial has no natural multivariate version, so full probability model approach to logistic regression is more challenging\nHowever, as previously with continuous outcome, the generalized estimating equations produce unbiased coefficient estimates under certain conditions\nSpecify working correlation matrix and use robust variance estimates\n\n\nlibrary(geepack); library(gee)\n\nrobglm.fit1 &lt;- geeglm(regsmoke ~ sex*age_c, data = smoke_prev, family =binomial,id=id)\nsummary(robglm.fit1)\n\n\nCall:\ngeeglm(formula = regsmoke ~ sex * age_c, family = binomial, data = smoke_prev, \n    id = id)\n\n Coefficients:\n                Estimate  Std.err    Wald Pr(&gt;|W|)    \n(Intercept)     -2.07908  0.08483 600.711  &lt; 2e-16 ***\nsexfemale        0.25049  0.11645   4.627   0.0315 *  \nage_c            0.26608  0.05950  19.996 7.76e-06 ***\nsexfemale:age_c  0.06319  0.07915   0.637   0.4247    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation structure = independence \nEstimated Scale Parameters:\n\n            Estimate Std.err\n(Intercept)   0.9986 0.09125\nNumber of clusters:   1867  Maximum cluster size: 6 \n\n\nCompare with normal logistic regression\n\n\nGEE\n\nrobglm.fit1 |&gt; \n  tbl_regression(intercept = TRUE,\n                 exponentiate = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\n(Intercept)\n0.13\n0.11, 0.15\n&lt;0.001\n\n\nsex\n\n\n\n\n\n\n\n\n    male\n—\n—\n\n\n\n\n    female\n1.28\n1.02, 1.61\n0.031\n\n\nage_c\n1.30\n1.16, 1.47\n&lt;0.001\n\n\nsex * age_c\n\n\n\n\n\n\n\n\n    female * age_c\n1.07\n0.91, 1.24\n0.4\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\n\n\nNormal Linear regression\n\nglm.fit1 |&gt; \n  tbl_regression(exponentiate = TRUE,\n                 intercept = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\n(Intercept)\n0.13\n0.11, 0.14\n&lt;0.001\n\n\nsex\n\n\n\n\n\n\n\n\n    male\n—\n—\n\n\n\n\n    female\n1.28\n1.13, 1.46\n&lt;0.001\n\n\nage_c\n1.30\n1.16, 1.47\n&lt;0.001\n\n\nsex * age_c\n\n\n\n\n\n\n\n\n    female * age_c\n1.07\n0.92, 1.24\n0.4\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs shown, the coefficient estimates are identical, but the 95% confidence interval is wider and the p-value is larger for the sex coefficient compared with the standard logistic regression. By using robust variance estimation, GEE accounts for potential within-individual correlation and answers the question, “If observations within an individual are correlated, how uncertain should the estimates be?” These results suggest reduced effective information for time-invariant covariates.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>GEE and GLMM for binary outcome</span>"
    ]
  }
]